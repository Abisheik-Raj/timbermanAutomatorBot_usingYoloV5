{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "517ee1ac-cded-44c8-a6a6-a39418523235",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyautogui\n",
    "import numpy as np\n",
    "import os\n",
    "import uuid\n",
    "import cv2\n",
    "import time\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df198e9-0e54-4e35-9fd7-4f24173992db",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/ultralytics/yolov5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d812bd44-e56c-46df-bb8f-9f94b08e4b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd yolov5 & pip install -r requirements.txt --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30394bcc-4b86-48f3-a2e1-6567b5e5e407",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04923bd3-29f8-4de4-8cfa-ac540dc36c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.hub.load(\"yolov5\",\"yolov5s\", source = \"local\",force_reload = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1696f3c0-fcc6-460b-bdda-9b4bafb7434f",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_FOLDER_PATH = os.path.join(\"data\",\"images_folder\")\n",
    "if os.name == \"nt\":  # Windows\n",
    "    os.makedirs(IMAGE_FOLDER_PATH, exist_ok=True)\n",
    "elif os.name == \"posix\":  # Linux or macOS\n",
    "    os.makedirs(IMAGE_FOLDER_PATH, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f483f6ef-4e89-4cd5-8993-04cac4588237",
   "metadata": {},
   "outputs": [],
   "source": [
    "#run this code for image collection\n",
    "no_of_images = 250\n",
    "time.sleep(10)\n",
    "print(\"Image Collection started!\")\n",
    "for no in range(no_of_images):\n",
    "    region = (150, 130, 650, 800) #adjust this according to your game screen size\n",
    "    screen = pyautogui.screenshot(region = region)\n",
    "    screen_np = np.array(screen)\n",
    "    screen_bgr = cv2.cvtColor(screen_np, cv2.COLOR_RGB2BGR)\n",
    "    IMAGE_FOLDER_PATH = os.path.join(\"data\",\"images\")\n",
    "    IMAGE_PATH = os.path.join(IMAGE_FOLDER_PATH, str(uuid.uuid1()) + \".jpg\")\n",
    "    cv2.imwrite(IMAGE_PATH,screen_bgr)\n",
    "    cv2.imshow(\"yolo\",screen_bgr) \n",
    "    print(\"Image no {} collected\".format(no + 1))\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a12ab6ad-f83e-45a5-b1fd-39eedd6c6c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now start labeling the images in data/images_folder\n",
    "#Use labelImg \n",
    "!pip install PyQt5\n",
    "!pip install lxml\n",
    "!git clone https://github.com/HumanSignal/labelImg.git\n",
    "!cd labelImg\n",
    "!pyrcc5 -o libs/resources.py resources.qrc\n",
    "!python labelImg.py #This will open up a application, using that label the images, save the labeled images in the same data/images directory\n",
    "#Remember to save the files in YOLO format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65576efc-4d13-4105-977a-cca2fcc7c669",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Before we start to train, we need to include two new files, \n",
    "# 1. Create a dataset.yaml file -> yolov5/dataset.yaml\n",
    "#Example dataset.yaml\n",
    "# path: C:\\Users\\Abisheik Raj\\OneDrive\\Desktop\\Image Detection Projects\\timberman_automation_bot\\data\n",
    "# train: images_folder\n",
    "# val: images_folder\n",
    "# test:\n",
    "\n",
    "# nc: 19s\n",
    "\n",
    "# names: [\n",
    "#     \"dog\", #0\n",
    "#     \"person\", #1\n",
    "#     \"cat\", #2\n",
    "#     \"tv\", #3\n",
    "#     \"car\", #4\n",
    "#     \"meatballs\", #5\n",
    "#     \"marinara sauce\", #6\n",
    "#     \"tomato soup\", #7\n",
    "#     \"chicken noodle soup\", #8\n",
    "#     \"french onion soup\", #9\n",
    "#     \"chicken breast\", #10\n",
    "#     \"ribs\", #11\n",
    "#     \"pulled pork\", #12\n",
    "#     \"hamburger\", #13\n",
    "#     \"cavity\", #14\n",
    "#     \"left_obs\", #15\n",
    "#     \"timberman\", #16\n",
    "#     \"right_obs\", #17\n",
    "#     \"lefet\", #18\n",
    "#   ]\n",
    "#for the names just copy paste the classes you have in data/images_folder/classes.txt\n",
    "\n",
    "# 2. Create a hyp.scratch.yaml and set fliplr: 0.0 to avoid the model to flip the labels, we do this to avoid fliping the left_obstacle and right_obstacle\n",
    "#Exampele: Create a hyp.scratch.yaml -> yolov5/hyp.scratch.yaml\n",
    "# lr0: 0.01 # initial learning rate (SGD=1E-2, Adam=1E-3)\n",
    "# lrf: 0.01 # final OneCycleLR learning rate (lr0 * lrf)\n",
    "# momentum: 0.937 # SGD momentum/Adam beta1\n",
    "# weight_decay: 0.0005 # optimizer weight decay 5e-4\n",
    "# warmup_epochs: 3.0 # warmup epochs (fractions ok)\n",
    "# warmup_momentum: 0.8 # warmup initial momentum\n",
    "# warmup_bias_lr: 0.1 # warmup initial bias lr\n",
    "# box: 0.05 # box loss gain\n",
    "# cls: 0.5 # cls loss gain\n",
    "# cls_pw: 1.0 # cls BCELoss positive_weight\n",
    "# obj: 1.0 # obj loss gain (scale with pixels)\n",
    "# obj_pw: 1.0 # obj BCELoss positive_weight\n",
    "# iou_t: 0.20 # IoU training threshold\n",
    "# anchor_t: 4.0 # anchor-multiple threshold\n",
    "# # anchors: 3  # anchors per output layer (0 to ignore)\n",
    "# fl_gamma: 0.0 # focal loss gamma (efficientDet default gamma=1.5)\n",
    "# hsv_h: 0.015 # image HSV-Hue augmentation (fraction)\n",
    "# hsv_s: 0.7 # image HSV-Saturation augmentation (fraction)\n",
    "# hsv_v: 0.4 # image HSV-Value augmentation (fraction)\n",
    "# degrees: 0.0 # image rotation (+/- deg)\n",
    "# translate: 0.1 # image translation (+/- fraction)\n",
    "# scale: 0.5 # image scale (+/- gain)\n",
    "# shear: 0.0 # image shear (+/- deg)\n",
    "# perspective: 0.0 # image perspective (+/- fraction), range 0-0.001\n",
    "# flipud: 0.0 # image flip up-down (probability)\n",
    "# fliplr: 0.0 # image flip left-right (probability)\n",
    "# mosaic: 1.0 # image mosaic (probability)\n",
    "# mixup: 0.0 # image mixup (probability)\n",
    "# copy_paste: 0.0 # segment copy-paste (probability)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e39cf021-ed18-49fc-bc52-635777dfc44c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This will train the model, this usually takes a lotta time if you only have a cpu, GPU recommended, more training hours == better model\n",
    "!cd yolov5 && python train.py --img 320 --batch 16 --epochs 2000 --data dataset.yaml --hyp hyp.scratch.yaml --weights yolov5s.pt --workers 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8326f76-ae77-480e-b6c1-84bb86232e8e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Load the model\n",
    "model = torch.hub.load('yolov5', 'custom', source='local', path='yolov5/runs/train/exp/weights/best.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f45685aa-7a2e-4a7c-ae49-c897b5b17ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run this script to test the model you have trained,\n",
    "while True:\n",
    "    region = (150, 500, 650, 500) #Change it according to your game screen size\n",
    "    screen = pyautogui.screenshot(region = region)\n",
    "    screen_np = np.array(screen)\n",
    "    screen_bgr = cv2.cvtColor(screen_np, cv2.COLOR_RGB2BGR)\n",
    "    result = model(screen_bgr)\n",
    "    cv2.imshow(\"yolo\",np.squeeze(result.render())) \n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f10d18-9b84-4ae2-8b1f-aeae8cb1bd0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This script will actually automate with the help of pyautogui and the object detection model we have fine-tuned\n",
    "CONFIDENCE_THRESHOLD = 0.3\n",
    "curr_position = \"left\"\n",
    "while True:\n",
    "    region = (150, 500, 650, 500)\n",
    "    screen = pyautogui.screenshot(region = region)\n",
    "    screen_np = np.array(screen)\n",
    "    screen_bgr = cv2.cvtColor(screen_np, cv2.COLOR_RGB2BGR)\n",
    "    result = model(screen_bgr)\n",
    "    cv2.imshow(\"yolo\",np.squeeze(result.render()))\n",
    "    result_coordinate = result.xyxy[0].cpu().numpy().tolist()  # Ensure itâ€™s a list\n",
    "    result_coordinate = list(reversed(result_coordinate)) \n",
    "\n",
    "    left_present = False\n",
    "    right_present = False\n",
    "    for res in result_coordinate:\n",
    "        x1, y1, x2, y2, confidence, cls = res\n",
    "        cls = int(cls)\n",
    "        if(confidence >= CONFIDENCE_THRESHOLD):\n",
    "            if(cls == 15):\n",
    "                left_present = True\n",
    "            elif(cls==16):\n",
    "                pass\n",
    "            elif(cls==17):\n",
    "                right_present = True\n",
    "\n",
    "    if left_present:\n",
    "        pyautogui.press(\"right\")\n",
    "        curr_position = \"right\"\n",
    "    elif right_present:\n",
    "        pyautogui.press(\"left\")\n",
    "        curr_position = \"left\"\n",
    "    else:\n",
    "        if curr_position == \"right\":\n",
    "            pyautogui.press(\"left\")\n",
    "        else:\n",
    "            pyautogui.press(\"right\") \n",
    "\n",
    "    time.sleep(0.1)\n",
    "    \n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d1374f-2d88-41cd-8e79-ba6e4282b02e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#NOTE: The dataset we have used here is from the online version of the game: https://html5games.com/Game/Timberman/a25e37b9-1550-49c9-b383-92ad51b4ecc2\n",
    "#So this model works well on the online version, if you want to try it out on the stream version, collect images from the stream game, label and retrain the model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "timberlake_bot",
   "language": "python",
   "name": "timberlake_bot"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
